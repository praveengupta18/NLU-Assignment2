{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from numpy import array\n",
    "from pickle import dump\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files=['austen-emma.txt','austen-persuasion.txt','austen-sense.txt','bible-kjv.txt','blake-poems.txt']\n",
    "guten = gutenberg.sents(files)\n",
    "\n",
    "gt_train,gt_test = train_test_split(list(guten),test_size=0.2,random_state=1)\n",
    "\n",
    "gt_dev,gt_test = train_test_split(gt_test,test_size=0.5,random_state=1)\n",
    "\n",
    "UNK = '<unknown>'\n",
    "gt_train[0].append(UNK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_words(data):\n",
    "    seq = []\n",
    "    for i in range(len(data)):\n",
    "        sen = [w.lower() for w in data[i] if w not in [':','/',';','|',\"''\",'``','(',')','-','--','_','\"',',','?'] if w.isalpha()]\n",
    "        seq += sen\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_less_frequent(data):\n",
    "    unks = set()\n",
    "    unique = set(data)\n",
    "    \n",
    "    uni_cfd = nltk.FreqDist(data)\n",
    "    for word,freq in uni_cfd.items():\n",
    "        if freq == 1:\n",
    "            unks.add(word)\n",
    "    \n",
    "    new_data = [w if w not in unks else UNK for w in data]\n",
    "            \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lines_from_list(data):\n",
    "    length = 30 + 1         \n",
    "    lines = list()\n",
    "    for i in range(length, int(len(data))):\n",
    "        seq = data[i-length:i]\n",
    "        line = ' '.join(seq)\n",
    "        lines.append(line)\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list_of_words(gt_train)\n",
    "ls = replace_less_frequent(ls)\n",
    "\n",
    "lines = lines_from_list(ls)\n",
    "\n",
    "data2 = '\\n'.join(lines)\n",
    "file = open('lines.txt', 'w')\n",
    "file.write(data2)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "lines = tokenizer.texts_to_sequences(lines)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lines = array(lines)\n",
    "X = lines[:,:-1]\n",
    "Y = lines[:,-1]\n",
    "Y = to_categorical(Y, num_classes=vocab_size)\n",
    "\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100, input_length=seq_length))\n",
    "model.add(LSTM(125, return_sequences=True))\n",
    "model.add(LSTM(125))\n",
    "model.add(Dense(125, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, Y, batch_size=400, epochs=45)\n",
    "\n",
    "model.save('word.h5')\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(test,tokenizer):\n",
    "    sen = [w if w in tokenizer.word_index else UNK for w in test if w not in [':','/',';','|',\"''\",'``','(',')','-','--','_','\"',',','?'] if w.isalpha()]\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.34\n"
     ]
    }
   ],
   "source": [
    "ls2 = list_of_words(gt_test)\n",
    "ls2 = test_data(ls2,tokenizer)\n",
    "\n",
    "lines2 = sequences_from_list(ls2)\n",
    "\n",
    "lines2 = tokenizer.texts_to_sequences(lines2)\n",
    "\n",
    "lines2 = array(lines2)\n",
    "X = lines2[:,:-1]\n",
    "Y = lines2[:,-1]\n",
    "Y = to_categorical(Y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]\n",
    "\n",
    "loss = model.evaluate(X,Y,batch_size=400)\n",
    "print(np.exp(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
